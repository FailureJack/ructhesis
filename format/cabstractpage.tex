% the abstract
\begin{abstractzh}
大语言模型（LLM）在多个领域的成功应用推动了人工智能发展，但其存在计算成本高、内存需求大等缺点，优化其推理效率、降低资源消耗成核心问题。主流大模型多为Decoder-Only架构，其中矩阵向量乘算子（GEMV）在大模型推理中占主导地位。GEMV因自身特点为内存瓶颈任务，GPU等计算密集型硬件利用率低，在边端场景下常成系统瓶颈。近存计算是新兴计算范式，能解决传统计算架构内存瓶颈问题。新兴的近存计算硬件，有高带宽、高存储、高并行性、高能效等优势，也存在计算能力弱、通信开销大等局限。如何从软件角度适配硬件加速，以及如何修改设计硬件优化其应用性能的软硬协同优化方案成为待深入研究的课题。

本论文基于近存计算技术研究算子GEMV的软硬协同加速方案：1）在近存计算硬件上，设计基于查找表的矩阵向量乘算法，使用查找表消除复杂计算，对近存计算硬件中的多级存储结构分别做出了访存优化，包括查找表分块算法减少通过DMA访问内存的次数，以及矩阵的行列重排减少对缓存的访问，增强寄存器的数据局部性；2）在模拟器上，为解决真实硬件上软件算法的计算瓶颈，基于周期精确近存激素模拟器设计硬件及指令以增强其计算能力，包括融合查表和加法指令用于高效快速查询乘积并累加，以及设计基于查找表的向量指令进行向量化的查表和访存，提高计算效率。最后对上述算法进行了详细的测试，分别在CPU和GPU以及近存硬件三个硬件平台上，进行了包括对GEMV算子总吞吐和能效比的测试。实验结果表明，在近存计算硬件上GEMV算子吞吐为355GOPS，大概达到理论性能的93\%，是CPU平台的14.7倍，能效比大概是CPU平台的8.6倍，是GPU平台的1.13倍。
\end{abstractzh}